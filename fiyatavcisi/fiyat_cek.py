import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

def trendyol_urun_ara(urun_adi):
    """Trendyol'da Ã¼rÃ¼n arama"""
    try:
        # TÃ¼rkÃ§e karakterleri dÃ¼zelt
        arama_terimi = urun_adi.replace(' ', '%20')
        url = f"https://www.trendyol.com/sr?q={arama_terimi}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        urunler = []
        
        # ÃœrÃ¼n kartlarÄ±nÄ± bul
        urun_kartlari = soup.find_all('div', {'class': 'p-card-wrppr'})[:5]  # Ä°lk 5 Ã¼rÃ¼n
        
        for kart in urun_kartlari:
            try:
                isim = kart.find('span', {'class': 'prdct-desc-cntnr-name'})
                isim = isim.text.strip() if isim else "Ä°sim bulunamadÄ±"
                
                fiyat = kart.find('div', {'class': 'prc-box-dscntd'})
                fiyat = fiyat.text.strip() if fiyat else "Fiyat bulunamadÄ±"
                
                urun_link = kart.find('a', {'class': 'p-card-chldrn-cntnr'})
                urun_link = 'https://www.trendyol.com' + urun_link['href'] if urun_link else "#"
                
                urunler.append({
                    'isim': isim,
                    'fiyat': fiyat,
                    'magaza': 'Trendyol',
                    'link': urun_link,
                    'tarih': datetime.now().strftime("%d.%m.%Y %H:%M")
                })
            except:
                continue
                
        return urunler
        
    except Exception as e:
        print("Hata:", e)
        return []

def hepsiburada_urun_ara(urun_adi):
    """Hepsiburada'da Ã¼rÃ¼n arama"""
    try:
        arama_terimi = urun_adi.replace(' ', '%20')
        url = f"https://www.hepsiburada.com/ara?q={arama_terimi}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        urunler = []
        urun_kartlari = soup.find_all('li', {'class': 'productListContent-zAP0Y5msy8OHn5z7T_K_'})[:5]
        
        for kart in urun_kartlari:
            try:
                isim = kart.find('h3', {'class': 'productListContent-zAP0Y5msy8OHn5z7T_K_'})
                isim = isim.text.strip() if isim else "Ä°sim bulunamadÄ±"
                
                fiyat = kart.find('div', {'data-test-id': 'price-current-price'})
                fiyat = fiyat.text.strip() if fiyat else "Fiyat bulunamadÄ±"
                
                urunler.append({
                    'isim': isim,
                    'fiyat': fiyat,
                    'magaza': 'Hepsiburada',
                    'link': url,
                    'tarih': datetime.now().strftime("%d.%m.%Y %H:%M")
                })
            except:
                continue
                
        return urunler
        
    except Exception as e:
        print("Hata:", e)
        return []

# Test iÃ§in
if __name__ == "__main__":
    print("ğŸ”„ ÃœrÃ¼nler aranÄ±yor...")
    
    # Ã–rnek Ã¼rÃ¼nler
    arama_urunleri = ["iphone", "laptop", "televizyon", "kulaklÄ±k"]
    
    tum_urunler = []
    
    for urun in arama_urunleri:
        print(f"ğŸ” {urun} aranÄ±yor...")
        
        # Trendyol'dan Ã¼rÃ¼nleri al
        trendyol_urunler = trendyol_urun_ara(urun)
        tum_urunler.extend(trendyol_urunler)
        
        # Hepsiburada'dan Ã¼rÃ¼nleri al
        hepsi_urunler = hepsiburada_urun_ara(urun)
        tum_urunler.extend(hepsi_urunler)
        
        # 2 saniye bekle (siteyi boÄŸmamak iÃ§in)
        import time
        time.sleep(2)
    
    # JSON dosyasÄ±na kaydet
    with open('fiyatlar.json', 'w', encoding='utf-8') as f:
        json.dump(tum_urunler, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {len(tum_urunler)} Ã¼rÃ¼n bulundu ve kaydedildi!")
